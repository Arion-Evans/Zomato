---
title: "Predicting Zomato restaurant ratings with logistic regression"
author: "Arion Barzoucas-Evans (s3650046) & Joshua Grosman (s3494389)"
date: "06/10/2018"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    fig_caption: yes
  word_document:
    toc: no
    toc_depth: '3'
  html_document:
    df_print: paged
    toc: no
    toc_depth: '3'
header-includes: \usepackage{float}
linkcolor: blue
documentclass: article
subtitle: MATH 1298 Analysis of Categorical Data Project Phase II
bibliography: references.bib
---


\newpage

\tableofcontents

\newpage

```{r, include=FALSE, message=FALSE, warning=FALSE}
# CAPTIONS
library(captioner)
fig_nums <- captioner(prefix = "Figure")
tab_nums <- captioner(prefix = "Table")

tab_nums(name = "data.preview")
tab_nums(name = "model.comp.bic")

fig_nums(name = "search.1.bic.plot", caption = "Fitted multinomial regression models and their BIC values.")


```

# Introduction \label{sec1}

During phase I of this project, exploratory data analysis was performed on data pertaining to 7,403 restaurants listed on Zomato (a restaurant search and discovery service founded in 2008). This included data pre-processing, creation of new variables, and visual representation of the data. According to this, the number of votes and whether a restaurant offers table bookings or not were found to be variables of particular interest. This phase of the project will focus on fitting a logistic regression model to the Zomato data and examining the effects of the different variables included in the model on a restaurant's rating.



\newpage

# Methodology

Following some final minor data preparation, a multinomial regression model will be fitted to the Zomato data. Firstly, the data will be split into training and test sets to enable unbaised accuracy assessments. The response variable for the model will be `Rating.text` having the following levels: "Poor", "Average", "Good", "Very Good", and "Excellent". There are a total of 7 explanatory independent variables in the dataset out of which only the most significant variables will be included in the model. This will be accomplished by performing an exhaustive search of all possible regression models and selecting the model with the lowest BIC value. The chosen predictors will then be included into a multinominal logistic regression model. Furthermore, as the response variable is ordinal, a proportional-odds cumulative logitistic model will also be constructed. The effect of the chosen predictors and overall model performance will then be examined and the best model will be identified based primarily on parameter significant and the residual deviance of the models. The test data will then be fed into model the predictions will compared to actual observations. The following `R` packages will be utilised to accomplice these tasks:

```{r, message = FALSE, warning=FALSE}
library(nnet)
library(car)
#library(MASS)
library(glmulti)
library(knitr)
library(captioner)
library(stargazer)
library(DescTools)
```


\newpage

# Data Preparation

`r tab_nums("data.preview", display = "cite")` displays the first 10 rows of the Zomato data. The levels for the `Rating.text` and `continent` were reordered so that "Poor" and "Rest of World" are the base levels for each variable respectively. This way 4 logistic regression models will be built, each comparing one level of the `Rating.text` variable to its base level ("Poor"). The data was then split into training and test sets using a 80:20 split via stratified sampling.

```{r}
zomato<-readRDS("zomato_phase2.rds")

kable(head(zomato, n = 10),
      caption = "First 10 rows of the Zomato dataset.")
```

```{r}


#Reordering factor variables to specify bases

#rating text (base=poor)
zomato$Rating.text<-factor(zomato$Rating.text, 
                           levels = c("Poor", "Average", "Good","Very Good", "Excellent")) 

#continent (base=rest of world)
zomato$continent<-factor(zomato$continent, 
                           levels = c("Rest of World", "Europe", "Asia","North America", "Oceania")) 

zomato$ID = seq(1,nrow(zomato),1) # for splitting dataset

#test/train

set.seed(57364)
zomato.train = zomato %>% group_by(Rating.text) %>% sample_frac(0.8) # stratified sampling
zomato.test = zomato[!(zomato$ID %in% zomato.train$ID),]

zomato.train = as.data.table(zomato.train)
zomato = zomato[,-c("ID")]
zomato.train = zomato.train[,-c("ID")]
zomato.test = zomato.test[,-c("ID")]

```

\newpage

# Feature Selection

```{r, eval=FALSE}
set.seed(123)
search.1way.bic <- glmulti(y = Rating.text ~ ., 
                         data = zomato.train, 
                         fitfunction = "glm", 
                         level = 1, 
                         method = "h", 
                         marginality = TRUE,
                         crit = "bic", 
                         family = binomial(link = "logit"))


aa <- weightable(search.1way.bic)
cbind(model = aa[1:5,1], round(aa[1:5,2:3], digits = 3))

plot(search.1way.bic, type = "p")
```

```{r, echo=FALSE}
search.1way.bic = readRDS("Feature Selection/search.1way.bic.rds")
aa <- weightable(search.1way.bic)
model.comp.bic = cbind(model = aa[1:5,1], round(aa[1:5,2:3], digits = 3))
kable(model.comp.bic,
      caption = "Top 5 multinomial regression models according to BIC.")
```

```{r, echo=FALSE}
plot(search.1way.bic, type = "p")
```
`r fig_nums("search.1.bic.plot")`

From this, it is clear that the best model to utilise should only include the `Has.Online.delivery` and `Votes` predictors. This small number of predictors is not surprising given that BIC favours smaller models.

\newpage

# Model Fitting

### Nominal Model

A multinominal logistic regression model was fitting using the variables identified in the previous section.

```{r, eval=FALSE}
fit.bic.nom<-multinom(formula = Rating.text ~ Has.Online.delivery  + Votes, data=zomato.train)
```

```{r, echo=FALSE}
fit.bic.nom<-readRDS("Models/bic_nom_TRAIN.rds")
```

The significance of the predictors was assessed below using an LRT for independence.

```{r}
Anova(fit.bic.nom)
```

The predictors are shown to be highly signficiant with p < .001 in both cases.

The coefficients of the 4 regression models are summarised below:

```{r, echo=FALSE}
summary(fit.bic.nom)
```

\newpage

### Ordinal Model

```{r,eval=FALSE}
fit.bic.ord<-polr(formula = Rating.text ~ Has.Online.delivery  + Votes, data=zomato.train,  method = "logistic")
```

```{r, echo=FALSE}
fit.bic.ord<-readRDS("Models/bic_ord_TRAIN.rds")
```

Again, an LRT was employed to check the signficance of predictors.

```{r, echo= FALSE}
Anova(fit.bic.ord)
```

Both predictors are shown to be highly significant at p < .001. Interestingly, the p-value for `Has.Online.Delivery` is higher in this model than the nominal model assessed previously, however here the value is still very small.

The model summary is depicted below:

```{r}
summary(fit.bic.ord)
```

## Model Comparison

Comparison of the nominal and ordinal models specified above suggested that the nominal model was superior. This conclusion is based mostly on the residual deviance of the models. The nominal model had a lower deviance of 14337.18, while the ordinal model's was 16230.23. Furthermore, it was seen that the `Has.Online.Delivery` predictor was slightly more significant within the nominal model compared to the ordinal one. As such, the nominal model was selected as the best model.


\newpage

# Final Model

The four logistic regression equations associated with the multinominal model defined previously are stated below:

* log(Pr(Average)/Pr(Poor)) = 4.014 - 1.179(Has.Online.Delivery) - 0.007(Votes)
* log(Pr(Good)/Pr(Poor)) = 2.263 - 1.120(Has.Online.Delivery) - 0.005(Votes)
* log(Pr(Very Good)/Pr(Poor)) = 1.345 - 1.768(Has.Online.Delivery) - 0.007(Votes)
* log(Pr(Excellent)/Pr(Poor)) = 0.039 - 2.766(Has.Online.Delivery) - 0.007(Votes)

# Paramter Confidence Intervals

```{r}
confint(fit.bic.nom, level=0.95)
```


# Effect of Independant Variables on Prediction Outcomes

Using the above model, odds ratios were generated to observe the effect that changing the two predictor variables independently had on the the response levels.

```{r}
fit.e = exp(coef(fit.bic.nom))
stargazer(fit.bic.nom, type = "text", coef=list(fit.e), p.auto=FALSE)
```

Here, it can be seen restaurants with online delivery are .307 times more likely than restaurants without online delivery to be rated average rather than poor. Furthermore, this trend is shown across all the levels, suggesting that generally, a restaurant with online delivery is more likely to be rated poor than any other level. For the Votes variable, a one unit increase in votes is shown to increase the odds of a restaurant being rated excellent as opposed to poor by 1.007. In fact, an increase in votes essentially translates into a restaurant being more likely to be rated anything other poor across all levels, except for the Average level.

\newpage

# Model Prediction

The effect of the predictor variables on the response level probabilities was further explored by feeding a dummy dataset into the model. The associated plot is shown below:

```{r, echo=FALSE}
#prediction dataframe
dvotes <- data.frame(Has.Online.delivery = rep(c("Yes", "No"), each = 50), 
                     Votes = rep(c(5, 125, 255, 505, 755, 1005, 1205, 1505, 2005, 2505,
                                   3005, 3505, 4005, 4505, 5005, 5505, 6005, 6505,7005,
                                   7505, 8005, 8505, 9005, 9505, 10005), 4))

#generate predictions using model
pp.votes <- cbind(dvotes, predict(fit.bic.nom, newdata = dvotes, type = "probs", se = TRUE))
#melt for visualisation
lpp <- melt(pp.votes, id.vars = c("Has.Online.delivery", "Votes"), value.name = "Probability")


ggplot(lpp, aes(x = Votes, y = Probability, colour = Has.Online.delivery)) + 
  geom_line() + facet_grid(variable ~ ., scales = "free") + theme_minimal()+
  labs(title="Probability of Rating Levels by Votes and Online Delivery")
```

\newpage

#Comparing Prediction Outcomes to Observations

The testing dataset was fed into the prediction model to obtain prediction response for the `Rating.text` variable. These results are compared to their associated actual observations in the plot below:

```{r, echo=FALSE}
zom.t<-zomato.test[,c("Rating.text","Has.Online.delivery","Votes")]

pred<-data.table(predict(fit.bic.nom,newdata = zom.t, type = "class"))
pred[, `:=` (Count = .N), by=V1]
pred<-data.frame(subset(unique(pred)))
pred$prop<-pred$Count/sum(pred$Count)

add<-data.frame("Poor",0,0)
names(add)<-c("V1","Count","prop")
pred<-rbind(pred,add)

#actual observations in same format as above
zom.p<-zom.t$Rating.text
zom.p<-data.table(zom.p)[, `:=` (Count = .N), by=zom.p]
zom.p<-data.frame(subset(unique(zom.p)))
zom.p$prop<-zom.p$Count/sum(zom.p$Count)

#matching variable names for merge
colnames(pred)[1]<-"rating"
colnames(zom.p)[1]<-"rating"

#merging and creating distinction variable
comp<-rbind(pred,zom.p)
comp$type<-rep(c("prediction","actual"),each = 5)

#actual vs predicted
ggplot(comp, aes(x=rating,y=prop,fill=type))+
  geom_bar(stat="identity",position = "dodge")+
  #geom_errorbar(aes(ymin=lwr.ci, ymax=upr.ci),position = "dodge")+#better in table?
  labs(title="Comparison of Predictions and Actual Observations",
       y="Proportion of Type",x="Rating")+theme_minimal()
```


# Summary


\newpage

# References

[Kaggle](https://www.kaggle.com/shrutimehta/zomato-restaurants-data)

---
nocite: | 
  @mlr, @data.table, @dplyr, @plyr, @ggplot2, @vcd, @knitr, @kableExtra,
...


