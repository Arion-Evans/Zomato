---
title: "Predicting Zomato restaurant ratings with logistic regression"
author: "Arion Barzoucas-Evans (s3650046) & Joshua Grosman (s3494389)"
date: "13/10/2018"
output:
  pdf_document:
    latex_engine: xelatex
    number_sections: yes
    fig_caption: yes
  word_document:
    toc: no
    toc_depth: '3'
  html_document:
    df_print: paged
    toc: no
    toc_depth: '3'
header-includes: \usepackage{float}
linkcolor: blue
documentclass: article
subtitle: MATH 1298 Analysis of Categorical Data Project Phase II
bibliography: references2.bib
---


\newpage

\tableofcontents

\newpage

```{r, include=FALSE, message=FALSE, warning=FALSE}
# CAPTIONS
library(captioner)
fig_nums <- captioner(prefix = "Figure")
tab_nums <- captioner(prefix = "Table")

tab_nums(name = "data.preview")
tab_nums(name = "model.comp.bic")

fig_nums(name = "search.1.bic.plot", caption = "Fitted multinomial regression models and their BIC values.")
fig_nums(name = "model.probs", caption = "Probability of each rating level by number of votes and whether the restaurant offers online delivery or not according to the multinomial logistic regression model.")
fig_nums(name = "model.pred", caption = "Bar plot comparing the proportion of the rating levels in the test dataset with the proportion of the rating levels of the predictions made by the model.")

```

# Introduction \label{sec1}

During phase I of this project, exploratory data analysis was performed on data pertaining to 7,403 restaurants listed on Zomato (a restaurant search and discovery service founded in 2008). This included data pre-processing, creation of new variables, and visual representation of the data. According to this, the number of votes and whether a restaurant offers table bookings or not were found to be variables of particular interest. This phase of the project will focus on fitting a logistic regression model to the Zomato data and examining the effects of the different variables included in the model on a restaurant's rating.



\newpage

# Methodology

Following some final minor data preparation, a multinomial logistic regression model will be fitted to the Zomato data. Firstly, the data will be split into training and test sets (80:20) to enable unbiased accuracy assessments. The response variable for the model will be `Rating.text` having the following levels: "Poor", "Average", "Good", "Very Good", and "Excellent". There are a total of 7 explanatory independent variables in the dataset out of which only the most significant variables will be included in the model. This will be accomplished by performing an exhaustive search of all possible regression models and selecting the model with the lowest BIC value. Due to computational constraints, feature selection will only consider one-way interactions between predictor variables. 

The chosen predictors will then be included into a multinomial logistic regression model. Furthermore, as the response variable is ordinal, a proportional-odds cumulative logistic model will also be constructed. The effect of the chosen predictors and overall model performance will then be examined, and the best model will be identified based primarily on parameter significance, and the residual deviance of the models. The relationship between the predictor variables and the probability of each response level will be further explored via examination of odds probabilities and visual representations. Finally, the test data will be fed into the model, and the predictions will be compared to actual observations. 

The following `R` packages will be utilised to accomplice these tasks:

```{r, message = FALSE, warning=FALSE}
library(nnet)
library(car)
library(glmulti)
library(knitr)
library(captioner)
library(stargazer)
library(data.table)
library(dplyr)
library(ggplot2)
library(png)
library(kableExtra)
```


\newpage

# Data Preparation

`r tab_nums("data.preview", display = "cite")` displays the first 10 rows of the Zomato data. The levels for the `Rating.text` and `continent` were reordered so that "Poor" and "Rest of World" are the base levels for each variable respectively. This way 4 logistic regression models will be built, each comparing one level of the `Rating.text` variable to its base level ("Poor"). The data was then split into training and test sets via stratified sampling.

```{r, echo=FALSE}
zomato<-readRDS("zomato_phase2.rds")

kable(head(zomato, n = 10),
      caption = "First 10 rows of the Zomato dataset.",
      format = "latex") %>% kable_styling(latex_options = c("scale_down","HOLD_position"))

```

```{r}


#Reordering factor variables to specify bases

#rating text (base=poor)
zomato$Rating.text<-factor(zomato$Rating.text, 
                           levels = c("Poor", "Average", "Good","Very Good", "Excellent")) 

#continent (base=rest of world)
zomato$continent<-factor(zomato$continent, 
                           levels = c("Rest of World", "Europe", 
                                      "Asia","North America", "Oceania")) 

zomato$ID = seq(1,nrow(zomato),1) # for splitting dataset

#test/train

set.seed(57364)
zomato.train = zomato %>% group_by(Rating.text) %>% sample_frac(0.8) # stratified sampling
zomato.test = zomato[!(zomato$ID %in% zomato.train$ID),]

zomato.train = as.data.table(zomato.train)
zomato = zomato[,-c("ID")]
zomato.train = zomato.train[,-c("ID")]
zomato.test = zomato.test[,-c("ID")]

```

\newpage

# Feature Selection

Next, the most important features in the dataset will be selected to be used in the multinomial regression model. To do this, an exhaustive search was performed where all possible logistic regression models (excluding 2-way interactions) were fitted and their BIC values were recorded. `r tab_nums("model.comp.bic", display = "cite")` displays the top 5 models fitted in terms of BIC while `r fig_nums("search.1.bic.plot", display = "cite")` shows all the fitted models and their BIC values. From this, it is clear that the best model to utilise should only include the `Has.Online.delivery` and `Votes` predictors. The small number of predictors is not surprising given that BIC favours smaller models.


```{r, eval=FALSE, echo=FALSE}
set.seed(123)
search.1way.bic <- glmulti(y = Rating.text ~ ., 
                         data = zomato.train, 
                         fitfunction = "glm", 
                         level = 1, 
                         method = "h", 
                         marginality = TRUE,
                         crit = "bic", 
                         family = binomial(link = "logit"))


aa <- weightable(search.1way.bic)
cbind(model = aa[1:5,1], round(aa[1:5,2:3], digits = 3))

plot(search.1way.bic, type = "p")
```

```{r, echo=FALSE}

aa <- readRDS("Feature Selection/1-way BIC/top5.1way.bic.rds")
model.comp.bic = cbind(model = aa[1:5,1], round(aa[1:5,2:3], digits = 3))
kable(model.comp.bic,
      caption = "Top 5 multinomial regression models according to BIC.")
```

```{r, echo=FALSE}
img <- readPNG("Feature Selection/1-way BIC/1way.BIC.plot.png")
grid::grid.raster(img)
```
`r fig_nums("search.1.bic.plot")`



\newpage

# Model Fitting

## Nominal Model

A multinomial logistic regression model was fitting using the variables identified in the previous section.

```{r, eval=FALSE}
fit.bic.nom<-multinom(formula = Rating.text ~ Has.Online.delivery  + Votes, 
                      data=zomato.train)
```

```{r, echo=FALSE}
fit.bic.nom<-readRDS("Models/bic_nom_TRAIN.rds")
```

\hfill\break

The significance of the predictors was assessed below using an LRT for independence.

```{r}
Anova(fit.bic.nom)
```

\hfill\break

The predictors are shown to be highly significant with p < .001 in both cases. The coefficients of the 4 regression models are summarised below:

```{r, echo=FALSE}
summary(fit.bic.nom)
```

\newpage

## Ordinal Model

A proportional-odds cumulative logistic model was also constructed using `Votes` and `Has.Online.delivery` as predictors.

```{r,eval=FALSE}
fit.bic.ord<-polr(formula = Rating.text ~ Has.Online.delivery  + Votes, 
                  data=zomato.train,  method = "logistic")
```

```{r, echo=FALSE}
fit.bic.ord<-readRDS("Models/bic_ord_TRAIN.rds")
```

\hfill\break

Again, an LRT was employed to check the significance of predictors.

```{r, warning=FALSE, message=FALSE}
Anova(fit.bic.ord)
```

\hfill\break

Both predictors are shown to be highly significant at p < .001. Interestingly, the p-value for `Has.Online.Delivery` is higher in this model than the nominal model assessed previously, however here the value is still very small.

The model summary is depicted below:

```{r}
summary(fit.bic.ord)
```

\newpage

## Model Comparison

Comparison of the nominal and ordinal models specified above suggested that the nominal model was superior. This conclusion is based mostly on the differences in the residual deviance of the models. The nominal model had a lower deviance of 11,559.63 , while the ordinal model's was 13,007.16. Furthermore, it was seen that the `Has.Online.Delivery` predictor was slightly more significant within the nominal model compared to the ordinal one. As such, the nominal model was selected as the best model.



## Final Model

The four logistic regression equations associated with the multinomial model defined previously are stated below:

\begin{itemize}  
\item $log(\frac{Pr(Average)}{Pr(Poor)}) = 4.014 - 1.179 \times Has.Online.Delivery - 0.007 \times Votes$
\item $log(\frac{Pr(Good)}{Pr(Poor)}) = 2.263 - 1.11 \times Has.Online.Delivery + 0.005 \times Votes$
\item $log(\frac{Pr(Very Good)}{Pr(Poor)}) = 1.345 - 1.768 \times Has.Online.Delivery + 0.007 \times Votes$ 
\item $log(\frac{Pr(Excellent)}{Pr(Poor)}) = 0.039 - 2.766 \times Has.Online.Delivery + 0.007 \times Votes$ 
\end{itemize}

### Parameter Confidence Intervals

The confidence intervals for the model's coefficients are shown below. It can be observed that all intervals are relatively narrow, and no interval contains 0.

```{r, echo=FALSE}
confint(fit.bic.nom, level=0.95)
```

\newpage

# Effect of Predictor Variables on Prediction Outcomes

## Odds

Using the above model, odds probabilities were generated to observe the effect of changing the two predictor variables independently on the response levels.

```{r}
fit.e = exp(coef(fit.bic.nom))
stargazer(fit.bic.nom, type = "text", coef=list(fit.e), p.auto=FALSE)
```

\hfill\break

Here, it can be seen restaurants with online delivery are .307 times more likely than restaurants without online delivery to be rated average rather than poor. Furthermore, this trend is shown across all the levels, suggesting that generally a restaurant with online delivery is more likely to be rated poor than any other level. For the Votes variable, a one unit increase in votes is shown to increase the odds of a restaurant being rated excellent as opposed to poor by 1.007. In fact, an increase in votes essentially translates into a restaurant being more likely to be rated anything other poor across all levels, except for the Average level where the odds are less than 1.

\newpage

## Relationship Between Predictors and Response Probability

The effect of the predictor variables on the response level probabilities was further explored by feeding a dummy dataset into the model. The associated plot is shown below:

```{r, echo=FALSE}
#prediction dataframe
dvotes <- data.frame(Has.Online.delivery = rep(c("Yes", "No"), each = 50), 
                     Votes = rep(c(5, 125, 255, 505, 755, 1005, 1205, 1505, 2005, 2505,
                                   3005, 3505, 4005, 4505, 5005, 5505, 6005, 6505,7005,
                                   7505, 8005, 8505, 9005, 9505, 10005), 4))

#generate predictions using model
pp.votes <- cbind(dvotes, predict(fit.bic.nom, newdata = dvotes, type = "probs", se = TRUE))
#melt for visualisation
lpp <- melt(pp.votes, id.vars = c("Has.Online.delivery", "Votes"), value.name = "Probability")


ggplot(lpp, aes(x = Votes, y = Probability, colour = Has.Online.delivery)) + 
  geom_line() + facet_grid(variable ~ ., scales = "free") + theme_minimal()+
  labs(title="Probability of Rating Levels by Votes and Online Delivery")
```
`r fig_nums("model.probs")`

\hfill\break

This visualisation provides valuable insight into how the predictors effect prediction outcomes. For example, it is clear that regardless of whether a restaurant has online delivery, if it has received over 5,000 votes, it is almost guaranteed to be rated as either very good or excellent. Furthermore, excellent restaurants are more likely to not offer online delivery regardless of how many votes are associated with it. This may be due to excellent-rated restaurant being "high-class" and therefore only offering in-house dining. 

Another interesting insight relates to the "poor" facet. Evidently there is much higher relative probability of a poor-rated restaurant having online delivery than not - apparently much higher than any other response level. This might explain why the odds calculated in the previous section suggested that restaurants which did offer online delivery were more likely to be rated poor than other level.

\newpage

# Model Prediction

The test dataset was fed into the prediction model to obtain prediction responses for the `Rating.text` variable. These results were compared to the associated actual observations in the plot below:

```{r, echo=FALSE}
zom.t<-zomato.test[,c("Rating.text","Has.Online.delivery","Votes")]

pred<-data.table(predict(fit.bic.nom,newdata = zom.t, type = "class"))
pred[, `:=` (Count = .N), by=V1]
pred<-data.frame(subset(unique(pred)))
pred$prop<-pred$Count/sum(pred$Count)

#need to add row as "poor" was not predicted at all
add<-data.frame("Poor",0,0)
names(add)<-c("V1","Count","prop")
pred<-rbind(pred,add)

#actual observations in same format as above
zom.p<-zom.t$Rating.text
zom.p<-data.table(zom.p)[, `:=` (Count = .N), by=zom.p]
zom.p<-data.frame(subset(unique(zom.p)))
zom.p$prop<-zom.p$Count/sum(zom.p$Count)

#matching variable names for merge
colnames(pred)[1]<-"rating"
colnames(zom.p)[1]<-"rating"

#merging and creating distinction variable
comp<-rbind(pred,zom.p)
comp$type<-rep(c("prediction","actual"),each = 5)

#actual vs predicted
ggplot(comp, aes(x=rating,y=prop,fill=type))+
  geom_bar(stat="identity",position = "dodge")+
  #geom_errorbar(aes(ymin=lwr.ci, ymax=upr.ci),position = "dodge")+#better in table?
  labs(title="Comparison of Predictions and Actual Observations",
       y="Proportion of Type",x="Rating")+theme_minimal()
```
`r fig_nums("model.pred")`

\hfill\break

This plot highlights that the model is not performing very well. While the proportion of the predicted "good" rating level is fairly close to the actual proportion, the proportion of predicted average-rated restaurants is much higher than the true proportion. Similarly, the proportion of predicted "very good" and "excellent" levels are much lower than true levels. The model did not predict any poor-rated restaurants. This issue may be related to the imbalance across the levels. This is further highlighted in the confusion matrix below where it can be seen that the model has mostly predicted an "Average" Rating for restaurants while it never made a "Poor" prediction and very few "Very Good" and "Excellent" predictions. This results in a mmce of 0.63.

```{r, echo=FALSE}
zom.t$pred = predict(fit.bic.nom,newdata = zom.t, type = "class")
table(zom.t$Rating.text,zom.t$pred, dnn = c("Actual","Predicted"))
```


\newpage

# Summary

Two logistic regression models were considered in order to predict Zomato restaurant ratings; a multinomial and a proportional odds logistic regression model. For both models, the most important features of the dataset were used. These were found to be the number of votes a restaurant has received and whether the restaurant offers online delivery or not. The multinomial logistic regression model was found to perform slighlty better than the proportional odds model and was used for the rest of the analysis. The equations associated with the multinomial model are:

\hfill\break

\begin{itemize}  
\item $log(\frac{Pr(Average)}{Pr(Poor)}) = 4.014 - 1.179 \times Has.Online.Delivery - 0.007 \times Votes$
\item $log(\frac{Pr(Good)}{Pr(Poor)}) = 2.263 - 1.11 \times Has.Online.Delivery + 0.005 \times Votes$
\item $log(\frac{Pr(Very Good)}{Pr(Poor)}) = 1.345 - 1.768 \times Has.Online.Delivery + 0.007 \times Votes$ 
\item $log(\frac{Pr(Excellent)}{Pr(Poor)}) = 0.039 - 2.766 \times Has.Online.Delivery + 0.007 \times Votes$ 
\end{itemize}

\hfill\break

According to this model, it was found that restaurants offering online delivery are more likely to be rated "Poor" than any other rating. Additionally, votes had a positive impact on a restaurant's rating as restaurants with more than 5,000 votes were almost guaranteed to have a "Very Good" or "Excellent" rating. However, in spite of these results, using 20% of the data as a test set, the model was found to be quite inaccurate in predicting the actual restaurant ratings. This could be attributed to the imbalance of the rating levels in the dataset as well as the potential existance of variables (not present in the dataset) more closely correlated to a restaurant's rating.

\newpage

# References

Data: [https://www.kaggle.com/shrutimehta/zomato-restaurants-data](https://www.kaggle.com/shrutimehta/zomato-restaurants-data)

---
nocite: | 
  @nnet, @car, @glmulti, @knitr, @captioner, @stargazer, @data.table, @dplyr, @ggplot2, @png, @kableExtra
...


